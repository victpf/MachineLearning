{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Policy with tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "# 1. Specify the neural network architecture\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 # it's a simple task, we don't need more hidden neurons\n",
    "n_outputs = 1 # only outputs the probability of accelerating left\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "# 2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,\n",
    "weights_initializer=initializer)\n",
    "logits = fully_connected(hidden, n_outputs, activation_fn=None,\n",
    "weights_initializer=initializer)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "# 3. Select a random action based on the estimated probabilities\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sub_2:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 1. - tf.to_float(action)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Cost Function and compute the grident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "'''Note that we are calling the optimizer’s compute_gradients() method\n",
    "instead of the minimize() method. This is because we want to tweak the\n",
    "gradients before we apply them.10 The compute_gradients() method returns\n",
    "a list of gradient vector/variable pairs (one pair per trainable variable). Let’s\n",
    "put all the gradients in a list, to make it more convenient to obtain their values:'''\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all the grident into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'gradients_1/fully_connected/MatMul_grad/tuple/control_dependency_1:0' shape=(4, 4) dtype=float32>,\n",
       " <tf.Tensor 'gradients_1/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0' shape=(4,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_1/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0' shape=(4, 1) dtype=float32>,\n",
       " <tf.Tensor 'gradients_1/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calucate the grident and choose the good one, save them into a vector and then feed back to the optimizat\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32,shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "#使用apply_gradients()函数来apply 经过选择更新的gradient\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "init = tf.global_variables_initializer()\n",
    "#保存已经训练的模型\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution phase\n",
    "    On to the execution phase! We will need a couple of functions to compute the\n",
    "    total discounted rewards, given the raw rewards, and to normalize the results\n",
    "    across multiple episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards,discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22., -40., -50.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([ 1.26665318,  1.0727777 ])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
